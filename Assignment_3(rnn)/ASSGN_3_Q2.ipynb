{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ASSGN_3_Q2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"EwHbwtb79CPT"},"source":["# Coded By Sayan Chandra\n","# Roll : CS20M057\n","# DEEP LEARNING (CS6910) ASSIGNMENT 3\n","# Instructor : Mitesh M. Khapra  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zk8598_ZVgmF"},"source":["def unzip():\n","    !tar -xf dakshina_dataset_v1.0.tar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uoo3r5SjVZy5"},"source":["def download():\n","    !wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0eJT0WYIVtn0"},"source":["def setWandb():\n","    global WANDB\n","    WANDB=0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U3sI4tYCkeay"},"source":["setWandb()\n","def download_unzip():\n","    download()\n","    unzip()\n","download_unzip()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gq9-RRSPkm4q"},"source":["if WANDB:\n","   !pip install --upgrade wandb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wz_60khtkmu4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621883363013,"user_tz":-330,"elapsed":5607,"user":{"displayName":"Sayan Chandra cs20m057","photoUrl":"","userId":"12052533651248196767"}},"outputId":"5dc31348-edeb-453f-c0e6-1141feb9f05f"},"source":["if WANDB: \n","  !wandb login #952756aa88ee3a472980bceb7d23632ac0a85500"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jj3oOheMkmqI"},"source":["import numpy as np\n","import tensorflow as tf\n","\n","if WANDB:\n","  import wandb\n","  from wandb.keras import WandbCallback as WC\n","  \n","from tensorflow import keras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Ybaz2pbkmlH"},"source":["sweep_config={\n","    'method' : 'bayes',\n","    'metric' : {\n","        'name' : 'val_accuracy(word_level)',\n","        'goal' : 'maximize',\n","    },\n","    'parameters' : {\n","        'input_embedding_size' : {\n","            'values' : [32, 64, 128],\n","        },\n","        'learning_rate' : {\n","            'values' : [1e-3, 2e-3, 3e-3],\n","        },\n","        'dropout' : {\n","            'values' : [0.2, 0.3, 0.5, 0.3]\n","        },\n","        'num_layers': { \n","            'values' : [1, 2, 3]\n","        },\n","        'hidden_layer_size' : {\n","            'values' : [128, 512, 256],\n","        },\n","        'cell_type' : {\n","            'values' : [ \"vanillaRnn\", \"lstm\", \"gru\"],\n","        },\n","        'optimizer' : {\n","            'values' : ['rmsprop', 'adam', 'sgd'],\n","        },\n","        'epochs' : {\n","            'values' : [6, 17, 13],\n","        },\n","        'batch_size' : {\n","            'values' : [32, 128, 64],\n","        },\n","    }\n","}\n","if WANDB: sweep_id = wandb.sweep(sweep_config, entity=\"blackcloud\", project=\"cs6910_dl_assignment_3\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0j5406rK0RY4"},"source":["def eid_did_dtd(qwerty):\n","  did = np.zeros((len(qwerty), max_decoder_seq_length), dtype=\"float32\") #2D  \n","  dtd = np.zeros((len(qwerty), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\") #3D\n","  eid = np.zeros((len(qwerty), max_encoder_seq_length), dtype=\"float32\") #2D\n","  return eid, did, dtd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Lb4rDwF01IV"},"source":["def bookKeep(qwerty):\n","    print(\"global vars are set for training the model.\")\n","    print(\"Num_samples:\", len(qwerty))\n","    print(\"# unique input tokens:\", num_encoder_tokens) # unique chars in english\n","    print(\"# unique output tokens:\", num_decoder_tokens) # unique chars in hindi\n","    print(\"Max sequence length for inputs:\", max_encoder_seq_length) # max wordlen in english\n","    print(\"Max sequence length for outputs:\", max_decoder_seq_length) # max wordlen in hindi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ckd5WMSHkyP_"},"source":["# GLOBAL VARS\n","num_encoder_tokens=0; num_decoder_tokens=0; \n","input_token_index={}; target_token_index={}\n","max_encoder_seq_length=0; max_decoder_seq_length=0;\n","input_characters=set()\n","target_characters = set()\n","##########################################################################################################################\n","\n","def preprocessing_TrainValTest(dataType, datapath, mode, isTrain, en_format):\n","  print(\"preprocessing started for\"+dataType)\n","  global max_encoder_seq_length, max_decoder_seq_length, input_token_index, target_token_index, num_encoder_tokens, num_decoder_tokens, input_characters, target_characters\n","  input_texts, target_texts = [], []\n","  with open(datapath, mode, encoding=en_format) as data:\n","    lines = data.read().split(\"\\n\")\n","  #print(len(lines))\n","  prev, __, _= lines[0].split(\"\\t\"); flag=1\n","  for iii, line in enumerate(lines[: len(lines) - 1]):\n","    target_text, input_text, ignore = line.split(\"\\t\")\n","    if iii>0 and target_text==prev : continue;\n","    prev=target_text\n","    #if target_text in target_texts : continue\n","    start = \"\\t\"; end=\"\\n\"\n","    target_text = start + target_text + end ;  input_text = start + input_text + end\n","    input_texts.append(input_text) ;   target_texts.append(target_text)\n","    if isTrain:\n","      for cc in input_text:\n","          if cc not in input_characters:\n","              input_characters.add(cc)\n","      for cc in target_text:\n","          if cc not in target_characters:\n","              target_characters.add(cc)\n","\n","\n","  if isTrain==1:\n","\n","    input_characters = sorted(list(input_characters))\n","    input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n","\n","    target_characters = sorted(list(target_characters))\n","    target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n","\n","    num_encoder_tokens = len(input_characters)\n","    max_encoder_seq_length = max([len(ii) for ii in input_texts])+1\n","\n","    num_decoder_tokens = len(target_characters)\n","    max_decoder_seq_length = max([len(ii) for ii in target_texts])+1 \n","\n","    bookKeep(input_texts)\n","\n","\n","\n","  encoder_input_data, decoder_input_data, decoder_target_data= eid_did_dtd(input_texts)\n","\n","  for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","      for t, char in enumerate(input_text):\n","          encoder_input_data[i, t] =  input_token_index[char]\n","      encoder_input_data[i, t + 1: ] = input_token_index[end]\n","      for t, char in enumerate(target_text):\n","          decoder_input_data[i, t] = target_token_index[char]\n","          if t > 0:\n","              decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n","      decoder_input_data[i, t + 1: ] = target_token_index[end]\n","      decoder_target_data[i, t:, target_token_index[end]] = 1.0\n","  print(\"preprocessing finished for\"+dataType+\"\\n\")\n","  return input_texts, target_texts, encoder_input_data, decoder_input_data, decoder_target_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"opDqPrVszb8y"},"source":["def setPaths():\n","    aaaaaa=\"dakshina_dataset_v1.0/hi/lexicons/\"\n","    bbbbbb =  aaaaaa+\"hi.translit.sampled.train.tsv\"\n","    cccccc =  aaaaaa+\"hi.translit.sampled.dev.tsv\"\n","    dddddd =  aaaaaa+\"hi.translit.sampled.test.tsv\"\n","    return aaaaaa, bbbbbb, cccccc, dddddd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jz7HSEFtkzHQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621603758944,"user_tz":-330,"elapsed":9,"user":{"displayName":"rik bubu","photoUrl":"","userId":"17850092690913342157"}},"outputId":"f0d9c4af-f19b-4095-cd97-de75da54ec51"},"source":["root, train_data_path, val_data_path, test_data_path = setPaths()\n","input_train_texts, target_train_texts, encoder_input_train_data, decoder_input_train_data, decoder_target_train_data = preprocessing_TrainValTest(\" train\", train_data_path, mode=\"r\", isTrain=1, en_format=\"utf-8\")\n","input_val_texts, target_val_texts, encoder_input_val_data, decoder_input_val_data, decoder_target_val_data = preprocessing_TrainValTest(\" val\" ,val_data_path, mode=\"r\", isTrain=0, en_format=\"utf-8\")\n","input_test_texts, target_test_texts, encoder_input_test_data, decoder_input_test_data, decoder_target_test_data = preprocessing_TrainValTest(\" test\" ,test_data_path, mode=\"r\", isTrain=0, en_format=\"utf-8\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["preprocessing started for train\n","global vars are set for training the model.\n","Num_samples: 25000\n","# unique input tokens: 28\n","# unique output tokens: 65\n","Max sequence length for inputs: 23\n","Max sequence length for outputs: 22\n","preprocessing finished for train\n","\n","preprocessing started for val\n","preprocessing finished for val\n","\n","preprocessing started for test\n","preprocessing finished for test\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NT7cKP9fhNSv"},"source":["def shapes():\n","   xx=keras.Input(shape=(max_encoder_seq_length,))\n","   yy=keras.Input(shape=(max_decoder_seq_length,))\n","   return xx, yy\n","\n","def makeModel(xxx, yyy):\n","   return keras.Model(xxx, yyy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sCo30ntHk2Te"},"source":["# RNN based seq2seq model\n","\n","keepCells = {\"gru\":keras.layers.GRU, \"lstm\":keras.layers.LSTM, \"vanillaRnn\":keras.layers.SimpleRNN, \"embedd\":keras.layers.Embedding, \"dense\":keras.layers.Dense}\n","\n","def ReccNeuralNet(en, input_embedd_sz, latent_dim, drop, cell):\n","\n","    # (i) input layer for character embeddings\n","    global putEncoders, HiddenStatesOfDecoder, putDecoders\n","\n","    encoder_inputs, decoder_inputs=shapes()\n","\n","    encoder_layer=keepCells[\"embedd\"](input_dim= num_encoder_tokens, output_dim=input_embedd_sz)\n","    encoder_embedded = encoder_layer(encoder_inputs)\n","    decoder_layer=keepCells[\"embedd\"](input_dim = num_decoder_tokens, output_dim=input_embedd_sz )\n","    decoder_embedded=decoder_layer(decoder_inputs)\n","    # (i) input layer for character embeddings\n","\n","\n","    putEncoders, putDecoders, HiddenStatesOfDecoder, en = [], [], [], en-1\n","    de=en\n","\n","\n","    # (ii) one encoder RNN which sequentially encodes the input character sequence\n","    if cell==\"vanillaRnn\":\n","\n","        ################################################################################################\n","        encoder = keepCells[cell](latent_dim , return_state=1, return_sequences=1)\n","        outputs, state_h = encoder(encoder_embedded)\n","        HiddenStatesOfDecoder.append([state_h])\n","        putEncoders.append(encoder)\n","\n","        for i in range(en):\n","          en_rnn = keepCells[cell](latent_dim , return_state=1, return_sequences=1, dropout=drop)\n","          outputs, state_h = en_rnn(outputs)\n","          HiddenStatesOfDecoder.append([state_h])\n","          putEncoders.append(en_rnn)\n","        ################################################################################################\n","\n","    elif cell==\"lstm\":\n","\n","        ################################################################################################\n","        encoder = keepCells[cell](latent_dim , return_state=1, return_sequences=1)\n","        outputs, state_h, state_c = encoder(encoder_embedded)\n","        HiddenStatesOfDecoder.append([state_h, state_c])\n","        putEncoders.append(encoder)\n","        for i in range(en):\n","          en_lstm =keepCells[cell](latent_dim , return_state=1, return_sequences=1, dropout=drop)\n","          outputs, state_h, state_c = en_lstm(outputs)\n","          HiddenStatesOfDecoder.append([state_h, state_c])\n","          putEncoders.append(en_lstm)\n","        ################################################################################################\n","\n","    elif cell==\"gru\":\n","\n","        ################################################################################################\n","        encoder = keepCells[cell](latent_dim , return_state=1, return_sequences=1)\n","        outputs, state_h = encoder(encoder_embedded)\n","        HiddenStatesOfDecoder.append([state_h])\n","        putEncoders.append(encoder)\n","        for i in range(en):\n","          en_gru = keepCells[cell](latent_dim , return_state=1, return_sequences=1, dropout=drop)\n","          outputs, state_h = en_gru(outputs)\n","          HiddenStatesOfDecoder.append([state_h])\n","          putEncoders.append(en_gru)\n","        ################################################################################################\n","\n","\n","    # (ii) one encoder RNN which sequentially encodes the input character sequence\n","     \n","\n","\n","\n","    # (iii) one decoder RNN which takes the last state of the encoder as input and produces one output character at a time\n","\n","    if cell==\"vanillaRnn\":\n","\n","        ################################################################################################\n","        decoder_rnn = keepCells[cell](latent_dim, return_sequences=1, return_state=1)\n","        de_outputs, ignore = decoder_rnn(decoder_embedded, initial_state=HiddenStatesOfDecoder[0])\n","        putDecoders.append(decoder_rnn)\n","        for i in range(de):\n","          de_rnn = keepCells[cell](latent_dim , return_state=1, return_sequences=1, dropout=drop)\n","          de_outputs, ignore = de_rnn(de_outputs,HiddenStatesOfDecoder[i+1])\n","          putDecoders.append(de_rnn)\n","        ################################################################################################\n","\n","    elif cell==\"lstm\":\n","\n","        ################################################################################################\n","        decoder_lstm = keepCells[cell](latent_dim, return_sequences=1, return_state=1)\n","        de_outputs, ignore1, ignore2 = decoder_lstm(decoder_embedded, initial_state=HiddenStatesOfDecoder[0])\n","        putDecoders.append(decoder_lstm)\n","        for i in range(de):\n","          de_lstm = keepCells[cell](latent_dim , return_state=1, return_sequences=1, dropout=drop)\n","          de_outputs, state_h, state_c = de_lstm(de_outputs, initial_state=HiddenStatesOfDecoder[i+1])\n","          putDecoders.append(de_lstm)\n","        ################################################################################################\n","\n","    elif cell==\"gru\":\n","\n","        ################################################################################################\n","        decoder_gru = keepCells[cell](latent_dim, return_sequences=1, return_state=1)\n","        de_outputs, ignore = decoder_gru(decoder_embedded, initial_state=HiddenStatesOfDecoder[0])\n","        putDecoders.append(decoder_gru)\n","        for i in range(de):\n","          de_gru = keepCells[cell](latent_dim , return_state=1, return_sequences=1, dropout=drop)\n","          de_outputs, ignore = de_gru(de_outputs, HiddenStatesOfDecoder[i+1])\n","          putDecoders.append(de_gru)\n","        ################################################################################################\n","\n","    # (iii) one decoder RNN which takes the last state of the encoder as input and produces one output character at a time\n","\n","\n","  \n","\n","    # Dense Layer\n","    makeDense=keepCells[\"dense\"](num_decoder_tokens, activation=\"softmax\")\n","    decoder_dense = makeDense\n","    decoder_outputs = decoder_dense(de_outputs) # None*22*65\n","    # Dense Layer\n","    \n","    #model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","    model = makeModel([encoder_inputs, decoder_inputs], decoder_outputs)\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y62uSEhTnZ8Y"},"source":["def inferenceModel (mdl, ld, emb_var, attn, cell):\n","  print(\"Model has been trained. Making inference model now.\")\n","  ld1,ld2=ld,ld\n","  input_to_encoder_embed = mdl.layers[0].output\n","  input_to_decoder_embed = mdl.layers[1].output\n","  en_embed_layer = mdl.layers[2]\n","  de_embed_layer = mdl.layers[3]\n","  en_embed_op = en_embed_layer(input_to_encoder_embed)\n","  de_embed_op= de_embed_layer(input_to_decoder_embed)\n","  statesEN = []\n","  outEN = en_embed_op  \n","  statesDEinps, statesDE = [], []\n","\n","  \n","  for i in range(len(ld1)):\n","      if cell == 'lstm':\n","        encoder_lstm = putEncoders[i]\n","        #print(outEN.shape)\n","        outEN, state_h, state_c = encoder_lstm(outEN)\n","        statesEN += [state_h, state_c]\n","      else:\n","        #print(\"yo\")\n","        encoder_gru_rnn = putEncoders[i]\n","        outEN, state_h = encoder_gru_rnn(outEN)\n","        statesEN += [state_h]\n","\n","  inp1=input_to_encoder_embed\n","  out2=statesEN + [outEN]\n","  encoder_model = keras.Model(inputs=inp1, outputs = out2)\n","\n","\n","\n","  for i in range(len(ld2)):\n","      if cell == 'lstm':\n","        statesCURinp = [keras.Input(shape=(ld2[i],)) for _ in range(2)]\n","        de_embed_op, state_h2, state_c2 = putDecoders[i](de_embed_op, initial_state=statesCURinp)\n","        statesDEinps += statesCURinp\n","        statesDE += [state_h2 , state_c2]\n","\n","      else:\n","        #print(\"yo\")\n","        statesCURinp = [keras.Input(shape=(ld2[i],)) for _ in range(1)]\n","        de_embed_op, state_h2=  putDecoders[i](de_embed_op, initial_state=statesCURinp)\n","        statesDEinps += statesCURinp\n","        statesDE += [state_h2 ]\n","        \n","  \n","  \n","  if attn == \"no\":\n","    print(\"This model is without attention.\")\n","    #Dense layer\n","    decoder_dense = mdl.layers[4 + len(ld1) + len(ld2)]\n","    hiddenstatesDEinp = keras.Input(shape=(max_encoder_seq_length , ld1[0]))\n","    de_embed_op = decoder_dense(de_embed_op) \n","\n","    # Define the decoder model\n","    inp3=[input_to_decoder_embed]  + statesDEinps + [hiddenstatesDEinp]\n","    out2=[de_embed_op] + [statesDE]\n","    decoder_model = keras.Model(inp3, out2) \n","\n","    print(\"Inference model is made now.\")\n","    return encoder_model , decoder_model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IoXelFWCm2Jg"},"source":["def beam_search(mtrx, k):\n","    from math import log\n","    from numpy import array, argmax\n","    mtrx  = tf.nn.softmax(mtrx)\n","    sequences = [[list(), 0.0]]\n","    for eachRow in mtrx:\n","        family = list()\n","        for i in range(len(sequences)):\n","            seq, marks= sequences[i]\n","            best_k = np.argsort(eachRow)[-k:]\n","            for j in best_k:\n","                person = [seq + [j], marks+ tf.math.log(eachRow[j])]\n","                family.append(person)\n","        ordered = sorted(family, key=lambda tup:tup[1], reverse=True)\n","        sequences = ordered[:k]\n","    return np.array(sequences)[:,0:1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S694UwkSoY3I"},"source":["reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n","def charByCharDecoding(input_seq):\n","\n","    global beamMatrix\n","    beamMatrix=[] # matrix for keeping probabilities of each char of a particular word\n","\n","    states_value = encoder_model.predict(input_seq) # predicting whole one-hot-encoded english word using encoder_model\n","\n","    curChar = np.zeros((1, 1)); start=\"\\t\"\n","    curChar[0, 0] = target_token_index[start]\n","    # empty target seq curChar with 1st char index = start char index = idx(\"\\t\") = 0\n","\n","    encoder_outputs=states_value[-1]\n","    encoder_states_value=states_value[:-1]\n","    # for decoder_model inputs\n","\n","    halt = False\n","    while not halt:\n","        output_decoder, decoder_states = decoder_model.predict([curChar] + encoder_states_value + [encoder_outputs])\n","        # predicting each char using decoder_model sequentially\n","\n","        beamMatrix.append(output_decoder[0][0])\n","        # append probabilities of char in beamMatrix\n","\n","        sampled_token_index = np.argmax(output_decoder)\n","        # max prob\n","\n","        if sampled_token_index == 1 :  halt = True\n","        # if char is \"\\n\" i.e. INDEX = 1 then break cond. = True\n","        \n","        curChar = np.zeros((1, 1))\n","        curChar[0, 0] = sampled_token_index\n","        # empty target seq curChar with 1st char = current char index\n","\n","        encoder_states_value = decoder_states\n","        # updation of prev with current\n","    return \"\"\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qaBCgaxRRYZr"},"source":["def WriteToCSVFile(arrays) :\n","  import csv\n","  f = open('/content/testPredictions.csv', 'w')\n","  for array in arrays:\n","      writer = csv.writer(f)\n","      writer.writerow(array)\n","  f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YDAoJ2AjoA8z"},"source":["import numpy as np\n","def summary_architechture(model):\n","  print(model.summary())\n","  for i in range (len(model.layers)):\n","     print(i, model.layers[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uMQqz-Jz-ppP"},"source":["def plotModel(mmm):\n","  return tf.keras.utils.plot_model(mmm , show_shapes = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4CIx0qyhn-J9"},"source":["class funcForWordAcc(tf.keras.callbacks.Callback):\n","  def __init__(self, strr, validation_data):\n","    self.strr=strr\n","    self.lsttt=[]\n","    self.asdfg = validation_data   \n","    \n","  def on_epoch_end(self, epoch, logs={}):\n","    \n","    ytrue = self.asdfg[1]\n","    ypred = self.model.predict(self.asdfg[0])\n","\n","    count = 0\n","    one_hot_pred = tf.one_hot(tf.argmax(ypred, axis=2), ypred.shape[2]).numpy()\n","    for i in range(len(ypred)):\n","      count+=np.array_equal(ytrue[i], one_hot_pred[i])\n","    xxxxx=count/len(ypred)\n","    print(self.strr+\"accword\", xxxxx)\n","    if WANDB: wandb.log({self.strr+\"_accuracy(word_level)\" : xxxxx, \"epoch\":epoch})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E-axeZcJk4Gw"},"source":["def TRAIN(model, opti, lr=1e-3, epk=15, bs=64):\n","    steps=1000\n","    from tensorflow.keras.experimental import CosineDecayRestarts as MyScheduler\n","    scheduler = (MyScheduler(lr, 1000))\n","\n","    if opti==\"adam\":  optimizer = keras.optimizers.Adam(learning_rate=scheduler)\n","    elif opti==\"sgd\": optimizer = keras.optimizers.SGD(learning_rate=scheduler)\n","    else : optimizer = keras.optimizers.RMSprop(learning_rate=scheduler)\n","\n","    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","    pcTrain = funcForWordAcc(\"train\", validation_data = ([encoder_input_train_data, decoder_input_train_data], decoder_target_train_data))\n","    pcVal = funcForWordAcc(\"val\", validation_data = ([encoder_input_val_data, decoder_input_val_data], decoder_target_val_data))\n","    \n","    history = model.fit(\n","        x=[encoder_input_train_data, decoder_input_train_data],\n","        y=decoder_target_train_data,\n","        shuffle=True,\n","        batch_size=bs,\n","        verbose=1,\n","        epochs=epk,\n","        validation_data= ([encoder_input_val_data, decoder_input_val_data], decoder_target_val_data),\n","        callbacks=([pcTrain, pcVal] if WANDB else [pcTrain, pcVal])\n","    )\n","    \n","    model.save(\"myrnnfinal\")\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vvo4BANMvPsR"},"source":["def WORDACCVAL(lengthOfPrediction=0, howMuch=0):\n","      n=len(encoder_input_val_data); \n","      for seq_index in range(n):\n","          input_seq = encoder_input_val_data[seq_index : seq_index + 1]\n","          ignore = charByCharDecoding(input_seq).replace(\"\\n\",\"\").strip()\n","          lengthOfPrediction=len(target_val_texts[seq_index].strip())\n","          result = beam_search(np.array(beamMatrix),3)\n","          target=decoder_input_val_data[seq_index,1:lengthOfPrediction+2]\n","          for seq in result:\n","            if np.array_equal(seq[0], target) :\n","              howMuch+=1; break\n","      print(\"val_inference_beam_acc\", howMuch/n)\n","      return howMuch/n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4IdyeTxNvMt8"},"source":["def Test_Acc_With_Best_Model_In_Sweep(lengthOfPrediction=0, howMuch=0):\n","      n=len(encoder_input_test_data);\n","      global arrays\n","      arrays=[['INPUT_WORD', 'PRED_WORD', 'TRUE_WORD']]\n","      print(\"calculation of Test inference accuracy has started...\")\n","      for seq_index in range(n):\n","          flag=1\n","          input_seq = encoder_input_test_data[seq_index : seq_index + 1]\n","          ignore = charByCharDecoding(input_seq).replace(\"\\n\",\"\").strip()\n","          lengthOfPrediction=len(target_test_texts[seq_index].strip())\n","          result = beam_search(np.array(beamMatrix),4)\n","          target=decoder_input_test_data[seq_index,1:lengthOfPrediction+2]\n","          for seq in result:\n","            if np.array_equal(seq[0], target) :\n","              ar=[]\n","              ar.append(input_test_texts[seq_index])\n","              ar.append(\"\".join([reverse_target_char_index[gg] for gg in seq[0]]).strip())\n","              ar.append(target_test_texts[seq_index][1:-1])\n","              arrays.append(ar)\n","              howMuch+=1; flag=0; break\n","          if flag: \n","              ar=[]\n","              ar.append(input_test_texts[seq_index])\n","              ar.append(\"\".join([reverse_target_char_index[gg] for gg in result[0][0]]).strip())\n","              ar.append(target_test_texts[seq_index][1:-1])\n","              arrays.append(ar)              \n","          if (seq_index) and seq_index%499==0 : print(\"so far accuracy = \",howMuch/(seq_index+1))\n","      print(\"test_inference_beam_acc\", howMuch/n)\n","      return howMuch/n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Upll6P4ok7x8"},"source":["#model = keras.models.load_model(\"myrnn\")\n","\n","\n","def SweepParent():\n","      global num_layers, input_embedding_size, hidden_layer_size, dropout, cell_type, encoder_model, decoder_model\n","      if WANDB:\n","        start=wandb.init()\n","        config=start.config\n","        a,b,c,d,e=config.num_layers, config.input_embedding_size, config.hidden_layer_size ,config.dropout, config.cell_type,\n","        f,g,h,i=config.optimizer, config.learning_rate, config.epochs, config.batch_size\n","        num_layers, input_embedding_size, hidden_layer_size, dropout, cell_type=a,b,c,d,e\n","        wandb.run.name=\"~/cell_\"+str(e)+\"/drop_\"+str(d)+\"/opti_\"+f+\"/bs_\"+str(i)+\"/epoch_\"+str(h)+\"~\"\n","        model=ReccNeuralNet(a,b,c,d,e)\n","        summary_architechture(model)\n","        model=TRAIN(model,f,g,h,i)\n","        encoder_model , decoder_model=inferenceModel(model, [hidden_layer_size for u in range(num_layers)],\n","                                                  [input_embedding_size, input_embedding_size], \"no\", cell_type) \n","        valAcc=WORDACCVAL()\n","        wandb.log({\"val_inference_beam_acc\":valAcc})\n","      else:\n","\n","        #{ best model from sweep runs ###\n","        a,b,c,d,e=2, 128, 256, 0.5, \"lstm\"\n","        f,g,h,i=\"rmsprop\", 3e-3, 17, 32\n","        model=ReccNeuralNet(a,b,c,d,e)\n","        #} ##############################\n","\n","        summary_architechture(model)\n","        num_layers, input_embedding_size, hidden_layer_size, dropout, cell_type=a,b,c,d,e\n","        model=TRAIN(model, f,g,h,i)\n","        encoder_model , decoder_model=inferenceModel(model, [hidden_layer_size for u in range(num_layers)], [input_embedding_size, input_embedding_size], \"no\", cell_type)  \n","      \n","      return model\n","      \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nKKwGzrik96n"},"source":["if WANDB: wandb.agent(\"ksbzk150\", SweepParent)\n","else: model=SweepParent() # best test accuracy I got = 0.5596 (55.96%) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yx-7EMymi2JA"},"source":["plotModel(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PZck6nDNuqCd","executionInfo":{"status":"ok","timestamp":1621603241233,"user_tz":-330,"elapsed":5726,"user":{"displayName":"rik bubu","photoUrl":"","userId":"17850092690913342157"}},"outputId":"47feaa22-d5b1-44b1-c053-30e4f24fdc44"},"source":["ip,prd,actual=[],[],[]\n","for seq_index in [2444, 449, 5, 1021, 2014, 784, 997, 981, 1090, 73, 85, 1884, 0, 700, 600, 850]:\n","    input_seq = encoder_input_test_data[seq_index : seq_index + 1]\n","    #print(input_seq)\n","    ignore = charByCharDecoding(input_seq) \n","    a=\"\"\n","    lst=[np.argmax(iii) for iii in beamMatrix]\n","    reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n","    for i in lst:\n","      a+=reverse_target_char_index[i]\n","    ip.append(input_test_texts[seq_index].strip()); \n","    prd.append(a.replace(\"\\n\",\"\").strip()); \n","    actual.append(target_test_texts[seq_index].strip())\n","from termcolor import colored\n","for i in range(4):\n","    print(colored(\"i/p:  \"+ip[i*4+0]+\"          i/p: \"+ ip[i*4+1]+ \"          i/p: \"+ ip[i*4+2]+ \"          i/p: \"+ ip[i*4+3],  'red', attrs=['bold']))\n","    print(colored(\"pred: \"+prd[i*4+0]+\"           pred: \"+prd[i*4+1]+ \"           pred: \"+ prd[i*4+2]+ \"           pred: \"+prd[i*4+3], 'blue', attrs=['bold']))\n","    print(colored(\"true: \" +actual[i*4+0]+\"           true: \"+actual[i*4+1]+ \"           true: \"+ actual[i*4+2]+ \"           true: \"+actual[i*4+3],  'green', attrs=['bold']))\n","\n","    print(\"\\n\\n\\n\")\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1m\u001b[31mi/p:  hakikat          i/p: kritika          i/p: angraji          i/p: dinaank\u001b[0m\n","\u001b[1m\u001b[34mpred: हकिकत           pred: कृतिका           pred: अंग्रजी           pred: दिनांक\u001b[0m\n","\u001b[1m\u001b[32mtrue: हकीकत           true: कृतिका           true: अंग्रज़ी           true: दिनांक\u001b[0m\n","\n","\n","\n","\n","\u001b[1m\u001b[31mi/p:  lenevali          i/p: junior          i/p: dastaanaa          i/p: thailee\u001b[0m\n","\u001b[1m\u001b[34mpred: लेनेवाली           pred: जूनियर           pred: दस्ताना           pred: ठैली\u001b[0m\n","\u001b[1m\u001b[32mtrue: लेनेवाली           true: जूनियर           true: दस्ताना           true: थैली\u001b[0m\n","\n","\n","\n","\n","\u001b[1m\u001b[31mi/p:  naksalavaad          i/p: amarpaal          i/p: arabab          i/p: ramkot\u001b[0m\n","\u001b[1m\u001b[34mpred: नकसलवाद           pred: अमरपाल           pred: अरबाब           pred: रामकोट\u001b[0m\n","\u001b[1m\u001b[32mtrue: नक्सलवाद           true: अमरपाल           true: अरबाब           true: रामकोट\u001b[0m\n","\n","\n","\n","\n","\u001b[1m\u001b[31mi/p:  ank          i/p: chhupata          i/p: ghumakkadi          i/p: toka\u001b[0m\n","\u001b[1m\u001b[34mpred: अंक           pred: छुपता           pred: घुमककदी           pred: टोका\u001b[0m\n","\u001b[1m\u001b[32mtrue: अंक           true: छुपता           true: घुमक्कड़ी           true: टोका\u001b[0m\n","\n","\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cv7goGwbm7HX","executionInfo":{"status":"ok","timestamp":1621603146452,"user_tz":-330,"elapsed":3654,"user":{"displayName":"rik bubu","photoUrl":"","userId":"17850092690913342157"}},"outputId":"2af7cb2a-1322-4340-956d-67b1bf20964e"},"source":["def wannaTry(check, a=\"\"):\n","    encodd= np.ones((1, max_encoder_seq_length), dtype=\"float32\"); space=\"                 \"\n","\n","    for i, input_text in enumerate([\"\\t\"+check+\"\\n\"]):\n","      for t, char in enumerate(input_text):\n","          encodd[i, t] =  input_token_index[char]\n","\n","    ignore=charByCharDecoding(encodd)\n","    lst=[np.argmax(iii) for iii in beamMatrix]\n","    reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n","    print(check.strip(), space, \"\".join([reverse_target_char_index[g] for g in lst]).replace(\"\\n\",\"\").strip())\n","\n","checks=[\"sayan\", \"chandra\", \"shivam\", \"varun\", \"amrita\", \"aanchal\", \"mayank\", \"mitesh\", \"khapra\", \"rahul\", \"bharat\", \"rasta\"]\n","print(\"input word:         predicted word:\")\n","print(\"-----------         ---------------\")\n","for check in checks:\n","  wannaTry(check)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["input word:         predicted word:\n","-----------         ---------------\n","sayan                   सायन\n","chandra                   चंद्र\n","shivam                   शिवाम\n","varun                   वरुण\n","amrita                   अमृता\n","aanchal                   आंचल\n","mayank                   मायंक\n","mitesh                   मिटेश\n","khapra                   खपरा\n","rahul                   राहुल\n","bharat                   भारत\n","rasta                   रस्ता\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MCWFOjGUe3z9"},"source":["Test_Acc_With_Best_Model_In_Sweep()\n","WriteToCSVFile(arrays)"],"execution_count":null,"outputs":[]}]}