{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ASSGN_3_Q1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"OPJQkxaT9lRl"},"source":["# Coded By Sayan Chandra\n","# Roll : CS20M057\n","# DEEP LEARNING (CS6910) ASSIGNMENT 3\n","# Instructor : Mitesh M. Khapra  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5gF3QrFWvCM4"},"source":["def unzip():\n","    !tar -xf dakshina_dataset_v1.0.tar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cIq2DQ-nvEEy"},"source":["def download():\n","    !wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IgcOQAwAERY6"},"source":["def download_unzip():\n","    download()\n","    unzip()\n","download_unzip()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PrdJ8p3vLeSB"},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9SEzLRkFvXQ6"},"source":["def eid_did_dtd(qwerty):\n","  did = np.zeros((len(qwerty), max_decoder_seq_length), dtype=\"float32\") #2D  \n","  dtd = np.zeros((len(qwerty), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\") #3D\n","  eid = np.zeros((len(qwerty), max_encoder_seq_length), dtype=\"float32\") #2D\n","  return eid, did, dtd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SHmJFYyTvZEF"},"source":["def bookKeep(qwerty):\n","    print(\"global vars are set for training the model.\")\n","    print(\"Num_samples:\", len(qwerty))\n","    print(\"# unique input tokens:\", num_encoder_tokens) # unique chars in english\n","    print(\"# unique output tokens:\", num_decoder_tokens) # unique chars in hindi\n","    print(\"Max sequence length for inputs:\", max_encoder_seq_length) # max wordlen in english\n","    print(\"Max sequence length for outputs:\", max_decoder_seq_length) # max wordlen in hindi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e_8v5m1cMJjA"},"source":["train_data_path = \"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n","val_data_path =   \"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n","test_data_path = \"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"etD0oW3ZMq5G"},"source":["# GLOBAL VARS\n","num_encoder_tokens=0; num_decoder_tokens=0; \n","input_token_index={}; target_token_index={}\n","max_encoder_seq_length=0; max_decoder_seq_length=0;\n","input_characters=set()\n","target_characters = set()\n","##########################################################################################################################\n","\n","def preprocessing_TrainValTest(dataType, datapath, mode, isTrain, en_format):\n","  print(\"preprocessing started for\"+dataType)\n","  global max_encoder_seq_length, max_decoder_seq_length, input_token_index, target_token_index, num_encoder_tokens, num_decoder_tokens, input_characters, target_characters\n","  input_texts, target_texts = [], []\n","  with open(datapath, mode, encoding=en_format) as data:\n","    lines = data.read().split(\"\\n\")\n","  #print(len(lines))\n","  prev, __, _= lines[0].split(\"\\t\"); flag=1\n","  for iii, line in enumerate(lines[: len(lines) - 1]):\n","    target_text, input_text, ignore = line.split(\"\\t\")\n","    if iii>0 and target_text==prev : continue;\n","    prev=target_text\n","    #if target_text in target_texts : continue\n","    start = \"\\t\"; end=\"\\n\"\n","    target_text = start + target_text + end ;  input_text = start + input_text + end\n","    input_texts.append(input_text) ;   target_texts.append(target_text)\n","    if isTrain:\n","      for cc in input_text:\n","          if cc not in input_characters:\n","              input_characters.add(cc)\n","      for cc in target_text:\n","          if cc not in target_characters:\n","              target_characters.add(cc)\n","\n","\n","  if isTrain==1:\n","\n","    input_characters = sorted(list(input_characters))\n","    input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n","\n","    target_characters = sorted(list(target_characters))\n","    target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n","\n","    num_encoder_tokens = len(input_characters)\n","    max_encoder_seq_length = max([len(ii) for ii in input_texts])+1\n","\n","    num_decoder_tokens = len(target_characters)\n","    max_decoder_seq_length = max([len(ii) for ii in target_texts])+1 \n","\n","    bookKeep(input_texts)\n","\n","\n","\n","  encoder_input_data, decoder_input_data, decoder_target_data= eid_did_dtd(input_texts)\n","\n","  for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","      for t, char in enumerate(input_text):\n","          encoder_input_data[i, t] =  input_token_index[char]\n","      encoder_input_data[i, t + 1: ] = input_token_index[end]\n","      for t, char in enumerate(target_text):\n","          decoder_input_data[i, t] = target_token_index[char]\n","          if t > 0:\n","              decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n","      decoder_input_data[i, t + 1: ] = target_token_index[end]\n","      decoder_target_data[i, t:, target_token_index[end]] = 1.0\n","  print(\"preprocessing finished for\"+dataType+\"\\n\")\n","  return input_texts, target_texts, encoder_input_data, decoder_input_data, decoder_target_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3E41W4bavdrc"},"source":["def setPaths():\n","    aaaaaa=\"dakshina_dataset_v1.0/hi/lexicons/\"\n","    bbbbbb =  aaaaaa+\"hi.translit.sampled.train.tsv\"\n","    cccccc =  aaaaaa+\"hi.translit.sampled.dev.tsv\"\n","    dddddd =  aaaaaa+\"hi.translit.sampled.test.tsv\"\n","    return aaaaaa, bbbbbb, cccccc, dddddd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QXb1OoWTWm9g"},"source":["root, train_data_path, val_data_path, test_data_path = setPaths()\n","input_train_texts, target_train_texts, encoder_input_train_data, decoder_input_train_data, decoder_target_train_data = preprocessing_TrainValTest(\" train\", train_data_path, mode=\"r\", isTrain=1, en_format=\"utf-8\")\n","input_val_texts, target_val_texts, encoder_input_val_data, decoder_input_val_data, decoder_target_val_data = preprocessing_TrainValTest(\" val\" ,val_data_path, mode=\"r\", isTrain=0, en_format=\"utf-8\")\n","input_test_texts, target_test_texts, encoder_input_test_data, decoder_input_test_data, decoder_target_test_data = preprocessing_TrainValTest(\" test\" ,test_data_path, mode=\"r\", isTrain=0, en_format=\"utf-8\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GbhAZIlGvk6j"},"source":["def shapes():\n","   xx=keras.Input(shape=(max_encoder_seq_length,))\n","   yy=keras.Input(shape=(max_decoder_seq_length,))\n","   return xx, yy\n","\n","def makeModel(xxx, yyy):\n","   return keras.Model(xxx, yyy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"19Yr5Y_DW2Sx"},"source":["# RNN based seq2seq model\n","\n","keepCells = {\"gru\":keras.layers.GRU, \"lstm\":keras.layers.LSTM, \"vanillaRnn\":keras.layers.SimpleRNN, \"embedd\":keras.layers.Embedding, \"dense\":keras.layers.Dense}\n","\n","def ReccNeuralNet(en, input_embedd_sz, latent_dim, drop, cell):\n","   \n","    global putEncoders, HiddenStatesOfDecoder, putDecoders\n","\n","\n","    # (i) input layer for character embeddings\n","\n","    encoder_inputs, decoder_inputs=shapes()\n","\n","    encoder_layer=keepCells[\"embedd\"](input_dim= num_encoder_tokens, output_dim=input_embedd_sz)\n","    encoder_embedded = encoder_layer(encoder_inputs)\n","    decoder_layer=keepCells[\"embedd\"](input_dim = num_decoder_tokens, output_dim=input_embedd_sz )\n","    decoder_embedded=decoder_layer(decoder_inputs)\n","    # (i) input layer for character embeddings\n","\n","\n","\n","    putEncoders, putDecoders, HiddenStatesOfDecoder, en = [], [], [], en-1\n","    de=en\n","\n","\n","\n","\n","    # (ii) one encoder RNN which sequentially encodes the input character sequence\n","    if cell==\"vanillaRnn\":\n","\n","        ################################################################################################\n","        encoder = keepCells[cell](latent_dim , return_state=1, return_sequences=1)\n","        outputs, state_h = encoder(encoder_embedded)\n","        HiddenStatesOfDecoder.append([state_h])\n","        putEncoders.append(encoder)\n","\n","        for i in range(en):\n","          en_rnn = keepCells[cell](latent_dim , return_state=1, return_sequences=1, dropout=drop)\n","          outputs, state_h = en_rnn(outputs)\n","          HiddenStatesOfDecoder.append([state_h])\n","          putEncoders.append(en_rnn)\n","        ################################################################################################\n","\n","    elif cell==\"lstm\":\n","\n","        ################################################################################################\n","        encoder = keepCells[cell](latent_dim , return_state=1, return_sequences=1)\n","        outputs, state_h, state_c = encoder(encoder_embedded)\n","        HiddenStatesOfDecoder.append([state_h, state_c])\n","        putEncoders.append(encoder)\n","        for i in range(en):\n","          en_lstm =keepCells[cell](latent_dim , return_state=1, return_sequences=1, dropout=drop)\n","          outputs, state_h, state_c = en_lstm(outputs)\n","          HiddenStatesOfDecoder.append([state_h, state_c])\n","          putEncoders.append(en_lstm)\n","        ################################################################################################\n","\n","    elif cell==\"gru\":\n","\n","        ################################################################################################\n","        encoder = keepCells[cell](latent_dim , return_state=1, return_sequences=1)\n","        outputs, state_h = encoder(encoder_embedded)\n","        HiddenStatesOfDecoder.append([state_h])\n","        putEncoders.append(encoder)\n","        for i in range(en):\n","          en_gru = keepCells[cell](latent_dim , return_state=1, return_sequences=1, dropout=drop)\n","          outputs, state_h = en_gru(outputs)\n","          HiddenStatesOfDecoder.append([state_h])\n","          putEncoders.append(en_gru)\n","        ################################################################################################\n","\n","\n","    # (ii) one encoder RNN which sequentially encodes the input character sequence\n","     \n","\n","\n","\n","    # (iii) one decoder RNN which takes the last state of the encoder as input and produces one output character at a time\n","\n","    if cell==\"vanillaRnn\":\n","\n","        ################################################################################################\n","        decoder_rnn = keepCells[cell](latent_dim, return_sequences=1, return_state=1)\n","        de_outputs, ignore = decoder_rnn(decoder_embedded, initial_state=HiddenStatesOfDecoder[0])\n","        putDecoders.append(decoder_rnn)\n","        for i in range(de):\n","          de_rnn = keepCells[cell](latent_dim , return_state=1, return_sequences=1, dropout=drop)\n","          de_outputs, ignore = de_rnn(de_outputs,HiddenStatesOfDecoder[i+1])\n","          putDecoders.append(de_rnn)\n","        ################################################################################################\n","\n","    elif cell==\"lstm\":\n","\n","        ################################################################################################\n","        decoder_lstm = keepCells[cell](latent_dim, return_sequences=1, return_state=1)\n","        de_outputs, ignore1, ignore2 = decoder_lstm(decoder_embedded, initial_state=HiddenStatesOfDecoder[0])\n","        putDecoders.append(decoder_lstm)\n","        for i in range(de):\n","          de_lstm = keepCells[cell](latent_dim , return_state=1, return_sequences=1, dropout=drop)\n","          de_outputs, state_h, state_c = de_lstm(de_outputs, initial_state=HiddenStatesOfDecoder[i+1])\n","          putDecoders.append(de_lstm)\n","        ################################################################################################\n","\n","    elif cell==\"gru\":\n","\n","        ################################################################################################\n","        decoder_gru = keepCells[cell](latent_dim, return_sequences=1, return_state=1)\n","        de_outputs, ignore = decoder_gru(decoder_embedded, initial_state=HiddenStatesOfDecoder[0])\n","        putDecoders.append(decoder_gru)\n","        for i in range(de):\n","          de_gru = keepCells[cell](latent_dim , return_state=1, return_sequences=1, dropout=drop)\n","          de_outputs, ignore = de_gru(de_outputs, HiddenStatesOfDecoder[i+1])\n","          putDecoders.append(de_gru)\n","        ################################################################################################\n","\n","    # (iii) one decoder RNN which takes the last state of the encoder as input and produces one output character at a time\n","\n","\n","  \n","\n","    # Dense Layer\n","    makeDense=keepCells[\"dense\"](num_decoder_tokens, activation=\"softmax\")\n","    decoder_dense = makeDense\n","    decoder_outputs = decoder_dense(de_outputs) # None*22*65\n","    # Dense Layer\n","    \n","    #model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","    model = makeModel([encoder_inputs, decoder_inputs], decoder_outputs)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LBmJjibzxc08"},"source":["import numpy as np\n","def summary_architechture(model):\n","  print(model.summary())\n","  for i in range (len(model.layers)):\n","     print(i, model.layers[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aLrYlSouxf5B"},"source":["def plotModel(mmm):\n","  return tf.keras.utils.plot_model(mmm , show_shapes = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5CwYnox1xUGJ"},"source":["a,b,c,d,e=2, 128, 256, 0.5, \"lstm\"\n","f,g,h,i=\"rmsprop\", 3e-3, 17, 32\n","model=ReccNeuralNet(a,b,c,d,e)\n","summary_architechture(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"puSV4lVNxgoS"},"source":["plotModel(model)"],"execution_count":null,"outputs":[]}]}